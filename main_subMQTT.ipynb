{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6b49453-7050-44b2-8235-24a9885debd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting paho-mqtt\n  Obtaining dependency information for paho-mqtt from https://files.pythonhosted.org/packages/c4/cb/00451c3cf31790287768bb12c6bec834f5d292eaf3022afc88e14b8afc94/paho_mqtt-2.1.0-py3-none-any.whl.metadata\n  Downloading paho_mqtt-2.1.0-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: azure-storage-blob in /databricks/python3/lib/python3.11/site-packages (12.19.1)\nRequirement already satisfied: azure-core<2.0.0,>=1.28.0 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob) (1.30.2)\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob) (41.0.3)\nRequirement already satisfied: typing-extensions>=4.3.0 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob) (4.10.0)\nRequirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob) (0.6.1)\nRequirement already satisfied: requests>=2.21.0 in /databricks/python3/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.31.0)\nRequirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.16.0)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.11/site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.15.1)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2023.7.22)\nDownloading paho_mqtt-2.1.0-py3-none-any.whl (67 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/67.2 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m67.2/67.2 kB\u001B[0m \u001B[31m3.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: paho-mqtt\nSuccessfully installed paho-mqtt-2.1.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Instalar paho-mqtt y la biblioteca de Azure Storage\n",
    "#%pip install paho-mqtt azure-storage-blob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1ccaa3b-26fd-4b41-b096-fa6863b0c3ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/2415/command-773880048719815-2944179060:21: DeprecationWarning: Callback API version 1 is deprecated, update to latest version\n  mqtt_client = mqtt.Client()\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<MQTTErrorCode.MQTT_ERR_SUCCESS: 0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import paho.mqtt.client as mqtt\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializar una lista para almacenar los mensajes\n",
    "mqtt_messages = []\n",
    "\n",
    "# Función que maneja los mensajes MQTT\n",
    "def on_message(client, userdata, message):\n",
    "    print(f\"Mensaje recibido en el tópico {message.topic}: {message.payload.decode()}\")\n",
    "    mqtt_messages.append((message.topic, message.payload.decode()))  # Guardar en lista\n",
    "\n",
    "# Configurar el cliente MQTT\n",
    "def on_connect(client, userdata, flags, rc):\n",
    "    if rc == 0:\n",
    "        print(\"Conexión exitosa al broker MQTT\")\n",
    "        client.subscribe(\"#\")  # Suscribirse a todos los tópicos\n",
    "    else:\n",
    "        print(f\"Error al conectarse al broker MQTT. Código de error: {rc}\")\n",
    "\n",
    "mqtt_client = mqtt.Client()\n",
    "mqtt_client.on_connect = on_connect\n",
    "mqtt_client.on_message = on_message\n",
    "\n",
    "# Establecer el usuario y la contraseña\n",
    "username = \"user_abv\"\n",
    "password = \"Tios1991m\"\n",
    "mqtt_client.username_pw_set(username, password)\n",
    "\n",
    "# Conectar al broker MQTT\n",
    "broker_address = \"0.tcp.eu.ngrok.io\"\n",
    "mqtt_client.connect(broker_address, 17966, 60)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c32762f9-6034-4138-8a25-063040395460",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión exitosa al broker MQTT\nMensaje recibido en el tópico ABV/huerto/tomate1: {\"Zona\":\"Tomate1\",\"Hora\":\"2024-09-17 19:00\",\"T_amb\":23.60000038,\"HR_amb\":62.29999924,\"HR_soil\":46,\"irrad\":103.9631424}\nMensaje recibido en el tópico ABV/huerto/tomate1: {\"Zona\":\"Tomate1\",\"Hora\":\"2024-09-17 19:01\",\"T_amb\":23.60000038,\"HR_amb\":62.09999847,\"HR_soil\":46,\"irrad\":104.3318024}\nMensaje recibido en el tópico ABV/huerto/tomate1: {\"Zona\":\"Tomate1\",\"Hora\":\"2024-09-17 19:02\",\"T_amb\":23.60000038,\"HR_amb\":62.29999924,\"HR_soil\":46,\"irrad\":103.5944748}\nMensaje recibido en el tópico ABV/huerto/tomate1: {\"Zona\":\"Tomate1\",\"Hora\":\"2024-09-17 19:03\",\"T_amb\":23.60000038,\"HR_amb\":62.20000076,\"HR_soil\":46,\"irrad\":104.5161285}\nMensaje recibido en el tópico ABV/huerto/tomate1: {\"Zona\":\"Tomate1\",\"Hora\":\"2024-09-17 19:04\",\"T_amb\":23.60000038,\"HR_amb\":62.29999924,\"HR_soil\":46,\"irrad\":103.9631424}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<MQTTErrorCode.MQTT_ERR_SUCCESS: 0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iniciar el loop para recibir mensajes en segundo plano\n",
    "mqtt_client.loop_start()\n",
    "\n",
    "# Esperar unos segundos para recibir mensajes\n",
    "import time\n",
    "time.sleep(5*60)  # Ajusta el tiempo según sea necesario\n",
    "\n",
    "# Detener el loop del cliente MQTT\n",
    "mqtt_client.loop_stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bea7ef0-c30e-496e-bd1a-571bf53c2ccb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n|             topic|             message|\n+------------------+--------------------+\n|ABV/huerto/tomate1|{\"Zona\":\"Tomate1\"...|\n|ABV/huerto/tomate1|{\"Zona\":\"Tomate1\"...|\n|ABV/huerto/tomate1|{\"Zona\":\"Tomate1\"...|\n|ABV/huerto/tomate1|{\"Zona\":\"Tomate1\"...|\n|ABV/huerto/tomate1|{\"Zona\":\"Tomate1\"...|\n+------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Crear un Spark DataFrame a partir de los mensajes recibidos\n",
    "spark = SparkSession.builder.appName(\"MQTT-Databricks\").getOrCreate()\n",
    "rdd = spark.sparkContext.parallelize(mqtt_messages)\n",
    "df = rdd.toDF([\"topic\", \"message\"])\n",
    "\n",
    "# Mostrar los datos recibidos\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4b572d4-55b4-436a-8719-99a61a1fcbc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "container = \"datos\"\n",
    "account = spark.conf.get(\"adls.account.name\")\n",
    "datos_path = f\"abfss://{container}@{account}.dfs.core.windows.net/datos-arduino\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "425496da-e724-4ece-9428-f95b14ee8dd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abfss://datos@masterabv003sta.dfs.core.windows.net/datos-arduino\n"
     ]
    }
   ],
   "source": [
    "print(datos_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baef51a1-2369-4bd9-8e5a-dcca6a81f9b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+-----------+-----------+-------+-----------+\n|   Zona|        DateTime|      T_amb|     HR_amb|HR_soil|      irrad|\n+-------+----------------+-----------+-----------+-------+-----------+\n|Tomate1|2024-09-17 19:00|23.60000038|62.29999924|     46|103.9631424|\n|Tomate1|2024-09-17 19:01|23.60000038|62.09999847|     46|104.3318024|\n|Tomate1|2024-09-17 19:02|23.60000038|62.29999924|     46|103.5944748|\n|Tomate1|2024-09-17 19:03|23.60000038|62.20000076|     46|104.5161285|\n|Tomate1|2024-09-17 19:04|23.60000038|62.29999924|     46|103.9631424|\n+-------+----------------+-----------+-----------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Función para leer datos del DataFrame en lugar de generarlos aleatoriamente\n",
    "def leer_datos_sensor(df):\n",
    "    # Inicializar una lista para almacenar los datos del sensor\n",
    "    datos_sensor = []\n",
    "\n",
    "    # Iterar sobre las filas del DataFrame para extraer los datos\n",
    "    for row in df.collect():  # Recuerda que 'df' es un DataFrame de Spark, así que usamos 'collect()' para obtener las filas\n",
    "        message = row['message']\n",
    "        data = json.loads(message)  # Convertir el mensaje JSON a un diccionario\n",
    "        \n",
    "        # Extraer los valores del diccionario\n",
    "        zona = data.get(\"Zona\")\n",
    "        hora = data.get(\"Hora\")\n",
    "        temperatura = data.get(\"T_amb\")\n",
    "        humedad_ambiente = data.get(\"HR_amb\")\n",
    "        humedad_suelo = data.get(\"HR_soil\")\n",
    "        radiacion = data.get(\"irrad\")\n",
    "\n",
    "        # Crear una tupla con los datos y añadirla a la lista\n",
    "        datos_sensor.append((zona, hora, temperatura, humedad_ambiente, humedad_suelo, radiacion))\n",
    "    \n",
    "    # Convertir la lista en un DataFrame\n",
    "    df_extract = spark.createDataFrame(datos_sensor, [\"Zona\", \"DateTime\", \"T_amb\", \"HR_amb\", \"HR_soil\", \"irrad\"])\n",
    "\n",
    "    # Retornar la lista de datos del sensor\n",
    "    return df_extract\n",
    "    \n",
    "df_extract = leer_datos_sensor(df)\n",
    "\n",
    "df_extract.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7aee6bb-fcca-4716-be18-0185827901d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def guardar_datos(df, datos_path):\n",
    "        df = df.repartition(1)\n",
    "        df.write.format('delta').mode(\"overwrite\").save(f\"{datos_path}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f434385-5978-47d9-ab52-96cf541d5da3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "guardar_datos(df_extract, datos_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main_subMQTT",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
